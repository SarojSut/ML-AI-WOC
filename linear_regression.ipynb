{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Training and testing data set(dataset is stored in coomputer) and altering data accodingly, these are done using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"D:\\DataSets\\linear_regression_train.csv\")\n",
    "test_data = pd.read_csv(\"D:\\DataSets\\linear_regression_test (1).csv\")\n",
    "train_data = train_data.drop(columns=[\"ID\"])\n",
    "test_data = test_data.drop(columns=[\"ID\"])\n",
    "print(train_data[:5])\n",
    "print(test_data[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Training data into features and target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_data.iloc[: ,0:25])\n",
    "Y_train = np.array(train_data.iloc[: ,25])\n",
    "\n",
    "print(f\"X_Training shape: {X_train.shape}\")\n",
    "print(f\"Y_Training shape: {Y_train.shape}\")\n",
    "print(f\"Test set shape: {test_data.shape}\")\n",
    "print(f\"Maximum value: {np.max(X_train)}\")\n",
    "print(f\"Minimum value: {np.min(X_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalizing the weights and bias terms.\n",
    "Ensure size of weights must be equal to number of features.\n",
    "Initalize learning rate = 0.01(0.01is a random pick any other value can also be used but its must be small)\n",
    "learning rate decides how much to adjust parameters weights and bias during training with respect to gradient loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(len(X_train[0]))\n",
    "b = 0\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of dataset is a crucial preprocessing step which improves the models performance.\n",
    "We need not normalize target values, if we do so then we have to denormalize the predicted values to get the actual predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    x_mean = X.mean()\n",
    "    x_std = X.std()\n",
    "    x_normal = (X-x_mean)/x_std\n",
    "    return x_normal\n",
    "\n",
    "X_train_normalize = normalize(X_train)\n",
    "Y_train_normalize = normalize(Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In  next cell, Gradient of cost function is being calculated.\n",
    "Cost function = -(1/2m)(summation)((z-Y[i])**2)\n",
    "derivative w.r.t w = ((z) - Y[i])*X[i]\n",
    "derivative w.r.t b = ((z) - Y[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, w, b):\n",
    "    m, n = X.shape\n",
    "    djdw = np.zeros(n)\n",
    "    djdb = 0.0\n",
    "    cost = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i], w) + b\n",
    "        err = z - Y[i]\n",
    "        cost += err**2 \n",
    "        # for j in range(n):\n",
    "        djdw += err*X[i]\n",
    "        djdb += err\n",
    "        \n",
    "    cost /= (2*m)\n",
    "    djdw /= m\n",
    "    djdb /= m\n",
    "    return djdw, djdb, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is, Gradient Descent updating the weights and biases after each Epoch and printing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, w, b):\n",
    "    Epoch = 0\n",
    "    loss_i = float('inf')\n",
    "    # while True:\n",
    "    for Epoch in range(2000):\n",
    "        # Epoch += 1\n",
    "        djdw, djdb, loss = gradient(X, Y, w, b)\n",
    "        rmse = np.sqrt(loss)\n",
    "        w -= learning_rate*djdw\n",
    "        b -= learning_rate*djdb\n",
    "        print(f\"Epoch: {Epoch+1} Loss: {rmse}\")\n",
    "        # if((loss_i-loss) <= 0.1):\n",
    "        #     break\n",
    "        # loss_i=loss\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_out, b_out = gradient_descent(X_train_normalize, Y_train, w, b)\n",
    "print(f\"Weights: {w_out}\\nBiase: {b_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is Denormaization code, though this is NOT used in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(original, pred):\n",
    "    original_mean = original.mean()\n",
    "    original_std = original.std()\n",
    "    de_normal = pred*original_std+original_mean\n",
    "    return de_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is just to check whether weights and bias are correct at some satisfactory level or not.\n",
    "Here, extracting the data from the training set, making predictions on that data using trained weights and bias.\n",
    "Though this is not required in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample():\n",
    "    y_pred = np.zeros(len(Y_train))\n",
    "    for i in range(len(X_train)):\n",
    "        y_pred[i] = np.dot(X_train_normalize[i],w_out) + b_out\n",
    "    # y_pred = denormalize(Y_train, y_pred)\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    print(f\"Y_train: {Y_train[:50]}\")\n",
    "    print(f\"y_pred: {y_pred[:50]}\")\n",
    "    \n",
    "sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, weights and bias are used on the test set and the predictions are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_normalize = normalize(np.array(test_data))\n",
    "test_prediction = np.zeros(len(test_data))\n",
    "for i in range(len(test_data)):\n",
    "    test_prediction[i] = np.dot(test_data_normalize[i],w_out) + b_out\n",
    "# test_prediction = denormalize(Y_train, y_pred)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(f\"y_pred: {test_prediction[:50]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
