{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\saroj\\AppData\\Local\\Temp\\ipykernel_21148\\54239383.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  train_data = pd.read_csv(\"D:\\DataSets\\linear_regression_train.csv\")\n",
      "C:\\Users\\saroj\\AppData\\Local\\Temp\\ipykernel_21148\\54239383.py:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  test_data = pd.read_csv(\"D:\\DataSets\\linear_regression_test (1).csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Feature_3   Feature_4  Feature_5  Feature_6  \\\n",
      "0 -10.429837  21.684251  93.056324  128.914740   1.657315 -34.602633   \n",
      "1  12.475081  11.652011 -47.621889  -21.892583 -25.844861 -46.951253   \n",
      "2   7.696188  24.729547  43.782383  -53.282523 -69.145794 -57.072335   \n",
      "3   6.505033  22.092605 -25.161422    4.742729 -20.344550 -50.248793   \n",
      "4   5.906445  19.909180  24.705865  -51.875251 -24.857500 -25.879977   \n",
      "\n",
      "    Feature_7   Feature_8   Feature_9  Feature_10  ...  Feature_17  \\\n",
      "0  -70.314374  -89.585318  -98.465642   32.708495  ...  -99.322780   \n",
      "1 -114.500805  -96.257473 -138.354981    3.507057  ...  -89.763107   \n",
      "2 -114.149171 -105.408739  -71.892560   66.893460  ...   -3.859655   \n",
      "3  -89.324731 -104.002016 -160.103875    7.199201  ...   48.557507   \n",
      "4  -37.787560  -99.587673  -47.929647   58.730086  ...    3.556973   \n",
      "\n",
      "   Feature_18  Feature_19  Feature_20  Feature_21  Feature_22  Feature_23  \\\n",
      "0   19.560135   25.568291  -40.876179 -116.146068  -82.181299  -99.423518   \n",
      "1   -8.250307   -8.798353  -23.452091  -56.769063  -26.722372  -97.694304   \n",
      "2  -17.301240   -1.622924  -39.351214  -73.973949  -33.844638 -100.874861   \n",
      "3   -1.997502   -0.856194  -34.259923   -3.113819   13.398035  -94.899866   \n",
      "4  -26.643253   -0.976567  -44.127974  -44.249542   -6.166813  -97.099111   \n",
      "\n",
      "   Feature_24  Feature_25       Target  \n",
      "0  -41.150654  113.117197  5235.690928  \n",
      "1   13.816113    8.711894 -1221.442146  \n",
      "2  -38.895598   34.488334 -1102.758862  \n",
      "3  -77.051331  -16.014027   437.940266  \n",
      "4   -5.426910  -42.191157   755.825367  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "   Feature_1  Feature_2  Feature_3   Feature_4  Feature_5  Feature_6  \\\n",
      "0 -13.296650  26.947913  95.977811  -25.167831 -25.863389 -46.432548   \n",
      "1  13.740478  26.632355  -1.056834  113.386950 -26.957189 -40.305571   \n",
      "2   5.757701   5.729282  -7.846318    9.063737 -54.522411 -17.102457   \n",
      "3  -0.365463  26.040174  15.941714  -47.019942 -32.029202 -34.080470   \n",
      "4  -7.076549  13.958214  52.805040  -41.259861 -22.072337 -42.073960   \n",
      "\n",
      "    Feature_7   Feature_8   Feature_9  Feature_10  ...  Feature_16  \\\n",
      "0  -30.120148  -99.220569  -95.771730  141.763723  ...  -46.275616   \n",
      "1   -8.916968  -99.182781  -65.653323   39.500793  ...  -53.961100   \n",
      "2  -89.391519  -81.827183 -104.007163   27.740454  ...  -31.822475   \n",
      "3 -105.277503 -104.742190 -135.598491    9.448577  ...   -6.004513   \n",
      "4  -42.290671  -91.763629  -18.252699   14.580324  ...  -38.963648   \n",
      "\n",
      "   Feature_17  Feature_18  Feature_19  Feature_20  Feature_21  Feature_22  \\\n",
      "0  -36.211021   46.588536   -1.146567  -55.504215  -70.993172  -13.516826   \n",
      "1  -27.153987  -20.942724   -5.648802  -27.809076 -151.023959  -37.020808   \n",
      "2  -89.093319   -9.672241   15.562814  -13.848764  -79.762693  -59.360365   \n",
      "3  -59.862724   16.676153  -14.092968  -30.554072  -86.409042   26.117858   \n",
      "4  -42.747877  -10.722687   18.179473  -39.178435 -149.742179  -36.540063   \n",
      "\n",
      "   Feature_23  Feature_24  Feature_25  \n",
      "0  -93.085585  -10.310519  -47.579967  \n",
      "1  -99.351817    7.806000   75.691539  \n",
      "2 -104.771868  -29.851237   71.920184  \n",
      "3  -97.644419  -57.080490    3.592127  \n",
      "4 -105.068864  -63.126882   41.053917  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"D:\\DataSets\\linear_regression_train.csv\")\n",
    "test_data = pd.read_csv(\"D:\\DataSets\\linear_regression_test (1).csv\")\n",
    "train_data = train_data.drop(columns=[\"ID\"])\n",
    "test_data = test_data.drop(columns=[\"ID\"])\n",
    "print(train_data[:5])\n",
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Training shape: (48000, 25)\n",
      "Y_Training shape: (48000,)\n",
      "Test set shape: (12000, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_data.iloc[: ,0:25])\n",
    "Y_train = np.array(train_data.iloc[: ,25])\n",
    "\n",
    "print(f\"X_Training shape: {X_train.shape}\")\n",
    "print(f\"Y_Training shape: {Y_train.shape}\")\n",
    "print(f\"Test set shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(len(X_train[0]))\n",
    "b = 0\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    x_mean = X.mean()\n",
    "    x_std = X.std()\n",
    "    x_normal = (X-x_mean)/x_std\n",
    "    return x_normal\n",
    "\n",
    "X_train_normalize = normalize(X_train)\n",
    "Y_train_normalize = normalize(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, w, b):\n",
    "    m, n = X.shape\n",
    "    djdw = np.zeros(n)\n",
    "    djdb = 0.0\n",
    "    cost = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i], w) + b\n",
    "        err = z - Y[i]\n",
    "        cost += err**2 \n",
    "        # for j in range(n):\n",
    "        djdw += err*X[i]\n",
    "        djdb += err\n",
    "        \n",
    "    cost /= (2*m)\n",
    "    djdw /= m\n",
    "    djdb /= m\n",
    "    return djdw, djdb, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, w, b):\n",
    "    Epoch = 0\n",
    "    loss_i = float('inf')\n",
    "    # while True:\n",
    "    for Epoch in range(2000):\n",
    "        # Epoch += 1\n",
    "        djdw, djdb, loss = gradient(X, Y, w, b)\n",
    "        rmse = np.sqrt(loss)\n",
    "        w -= learning_rate*djdw\n",
    "        b -= learning_rate*djdb\n",
    "        print(f\"Epoch: {Epoch+1} Loss: {rmse}\")\n",
    "        # if((loss_i-loss) <= 0.1):\n",
    "        #     break\n",
    "        # loss_i=loss\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1761.6045631443124\n",
      "Epoch: 2 Loss: 1680.190689222812\n",
      "Epoch: 3 Loss: 1624.504074030637\n",
      "Epoch: 4 Loss: 1586.44336199397\n",
      "Epoch: 5 Loss: 1560.234360779798\n",
      "Epoch: 6 Loss: 1541.887699395288\n",
      "Epoch: 7 Loss: 1528.709015504104\n",
      "Epoch: 8 Loss: 1518.9072604396276\n",
      "Epoch: 9 Loss: 1511.3046553191018\n",
      "Epoch: 10 Loss: 1505.1322722006194\n",
      "Epoch: 11 Loss: 1499.8904871861575\n",
      "Epoch: 12 Loss: 1495.255664049447\n",
      "Epoch: 13 Loss: 1491.0185798768994\n",
      "Epoch: 14 Loss: 1487.0441445725457\n",
      "Epoch: 15 Loss: 1483.2451968614187\n",
      "Epoch: 16 Loss: 1479.5655189314537\n",
      "Epoch: 17 Loss: 1475.9688521970845\n",
      "Epoch: 18 Loss: 1472.4318046152107\n",
      "Epoch: 19 Loss: 1468.9392752978172\n",
      "Epoch: 20 Loss: 1465.4815048600137\n",
      "Epoch: 21 Loss: 1462.0521746233212\n",
      "Epoch: 22 Loss: 1458.6471820429824\n",
      "Epoch: 23 Loss: 1455.263851924152\n",
      "Epoch: 24 Loss: 1451.9004284012087\n",
      "Epoch: 25 Loss: 1448.55574776878\n",
      "Epoch: 26 Loss: 1445.2290277931863\n",
      "Epoch: 27 Loss: 1441.9197320391065\n",
      "Epoch: 28 Loss: 1438.6274825042692\n",
      "Epoch: 29 Loss: 1435.3520033624309\n",
      "Epoch: 30 Loss: 1432.0930847382763\n",
      "Epoch: 31 Loss: 1428.8505593810582\n",
      "Epoch: 32 Loss: 1425.6242876444508\n",
      "Epoch: 33 Loss: 1422.4141478143085\n",
      "Epoch: 34 Loss: 1419.2200298803962\n",
      "Epoch: 35 Loss: 1416.0418315254803\n",
      "Epoch: 36 Loss: 1412.879455542037\n",
      "Epoch: 37 Loss: 1409.7328081685819\n",
      "Epoch: 38 Loss: 1406.6017980176862\n",
      "Epoch: 39 Loss: 1403.4863353852804\n",
      "Epoch: 40 Loss: 1400.3863318052456\n",
      "Epoch: 41 Loss: 1397.30169976222\n",
      "Epoch: 42 Loss: 1394.2323525058944\n",
      "Epoch: 43 Loss: 1391.178203931042\n",
      "Epoch: 44 Loss: 1388.1391684995604\n",
      "Epoch: 45 Loss: 1385.115161189769\n",
      "Epoch: 46 Loss: 1382.1060974631566\n",
      "Epoch: 47 Loss: 1379.1118932424752\n",
      "Epoch: 48 Loss: 1376.1324648968903\n",
      "Epoch: 49 Loss: 1373.1677292320223\n",
      "Epoch: 50 Loss: 1370.2176034828833\n",
      "Epoch: 51 Loss: 1367.2820053087698\n",
      "Epoch: 52 Loss: 1364.3608527893803\n",
      "Epoch: 53 Loss: 1361.4540644218062\n",
      "Epoch: 54 Loss: 1358.5615591179958\n",
      "Epoch: 55 Loss: 1355.6832562025345\n",
      "Epoch: 56 Loss: 1352.8190754107347\n",
      "Epoch: 57 Loss: 1349.9689368867282\n",
      "Epoch: 58 Loss: 1347.1327611818497\n",
      "Epoch: 59 Loss: 1344.3104692527938\n",
      "Epoch: 60 Loss: 1341.5019824601222\n",
      "Epoch: 61 Loss: 1338.7072225666552\n",
      "Epoch: 62 Loss: 1335.9261117357435\n",
      "Epoch: 63 Loss: 1333.1585725299126\n",
      "Epoch: 64 Loss: 1330.404527909124\n",
      "Epoch: 65 Loss: 1327.663901229303\n",
      "Epoch: 66 Loss: 1324.9366162407975\n",
      "Epoch: 67 Loss: 1322.2225970867885\n",
      "Epoch: 68 Loss: 1319.5217683018343\n",
      "Epoch: 69 Loss: 1316.8340548102667\n",
      "Epoch: 70 Loss: 1314.1593819247175\n",
      "Epoch: 71 Loss: 1311.4976753445446\n",
      "Epoch: 72 Loss: 1308.8488611543892\n",
      "Epoch: 73 Loss: 1306.2128658226075\n",
      "Epoch: 74 Loss: 1303.5896161997596\n",
      "Epoch: 75 Loss: 1300.9790395171553\n",
      "Epoch: 76 Loss: 1298.3810633852884\n",
      "Epoch: 77 Loss: 1295.7956157924002\n",
      "Epoch: 78 Loss: 1293.2226251029356\n",
      "Epoch: 79 Loss: 1290.6620200560803\n",
      "Epoch: 80 Loss: 1288.1137297642897\n",
      "Epoch: 81 Loss: 1285.5776837117535\n",
      "Epoch: 82 Loss: 1283.053811752965\n",
      "Epoch: 83 Loss: 1280.5420441112306\n",
      "Epoch: 84 Loss: 1278.0423113771922\n",
      "Epoch: 85 Loss: 1275.5545445073615\n",
      "Epoch: 86 Loss: 1273.0786748226597\n",
      "Epoch: 87 Loss: 1270.6146340069372\n",
      "Epoch: 88 Loss: 1268.162354105532\n",
      "Epoch: 89 Loss: 1265.721767523834\n",
      "Epoch: 90 Loss: 1263.29280702578\n",
      "Epoch: 91 Loss: 1260.8754057324406\n",
      "Epoch: 92 Loss: 1258.4694971205702\n",
      "Epoch: 93 Loss: 1256.075015021182\n",
      "Epoch: 94 Loss: 1253.6918936180477\n",
      "Epoch: 95 Loss: 1251.3200674463233\n",
      "Epoch: 96 Loss: 1248.9594713911092\n",
      "Epoch: 97 Loss: 1246.6100406859848\n",
      "Epoch: 98 Loss: 1244.2717109116188\n",
      "Epoch: 99 Loss: 1241.9444179943068\n",
      "Epoch: 100 Loss: 1239.6280982046044\n",
      "Epoch: 101 Loss: 1237.3226881558473\n",
      "Epoch: 102 Loss: 1235.028124802803\n",
      "Epoch: 103 Loss: 1232.7443454401846\n",
      "Epoch: 104 Loss: 1230.4712877013176\n",
      "Epoch: 105 Loss: 1228.2088895566856\n",
      "Epoch: 106 Loss: 1225.9570893125249\n",
      "Epoch: 107 Loss: 1223.7158256094713\n",
      "Epoch: 108 Loss: 1221.485037421112\n",
      "Epoch: 109 Loss: 1219.2646640526116\n",
      "Epoch: 110 Loss: 1217.0546451393443\n",
      "Epoch: 111 Loss: 1214.8549206454652\n",
      "Epoch: 112 Loss: 1212.665430862528\n",
      "Epoch: 113 Loss: 1210.4861164081728\n",
      "Epoch: 114 Loss: 1208.3169182246331\n",
      "Epoch: 115 Loss: 1206.1577775774633\n",
      "Epoch: 116 Loss: 1204.0086360540952\n",
      "Epoch: 117 Loss: 1201.8694355625144\n",
      "Epoch: 118 Loss: 1199.7401183298555\n",
      "Epoch: 119 Loss: 1197.620626901049\n",
      "Epoch: 120 Loss: 1195.5109041374856\n",
      "Epoch: 121 Loss: 1193.410893215622\n",
      "Epoch: 122 Loss: 1191.3205376256383\n",
      "Epoch: 123 Loss: 1189.2397811700805\n",
      "Epoch: 124 Loss: 1187.1685679625239\n",
      "Epoch: 125 Loss: 1185.1068424262016\n",
      "Epoch: 126 Loss: 1183.054549292665\n",
      "Epoch: 127 Loss: 1181.0116336004883\n",
      "Epoch: 128 Loss: 1178.9780406938426\n",
      "Epoch: 129 Loss: 1176.953716221225\n",
      "Epoch: 130 Loss: 1174.9386061341195\n",
      "Epoch: 131 Loss: 1172.932656685619\n",
      "Epoch: 132 Loss: 1170.9358144291693\n",
      "Epoch: 133 Loss: 1168.9480262171842\n",
      "Epoch: 134 Loss: 1166.9692391997307\n",
      "Epoch: 135 Loss: 1164.9994008232488\n",
      "Epoch: 136 Loss: 1163.0384588291947\n",
      "Epoch: 137 Loss: 1161.0863612527423\n",
      "Epoch: 138 Loss: 1159.143056421461\n",
      "Epoch: 139 Loss: 1157.208492954046\n",
      "Epoch: 140 Loss: 1155.2826197589443\n",
      "Epoch: 141 Loss: 1153.3653860331194\n",
      "Epoch: 142 Loss: 1151.456741260718\n",
      "Epoch: 143 Loss: 1149.5566352117528\n",
      "Epoch: 144 Loss: 1147.6650179408757\n",
      "Epoch: 145 Loss: 1145.7818397860087\n",
      "Epoch: 146 Loss: 1143.9070513671052\n",
      "Epoch: 147 Loss: 1142.0406035848675\n",
      "Epoch: 148 Loss: 1140.1824476194179\n",
      "Epoch: 149 Loss: 1138.3325349290762\n",
      "Epoch: 150 Loss: 1136.49081724903\n",
      "Epoch: 151 Loss: 1134.6572465901083\n",
      "Epoch: 152 Loss: 1132.8317752374683\n",
      "Epoch: 153 Loss: 1131.0143557493761\n",
      "Epoch: 154 Loss: 1129.2049409558886\n",
      "Epoch: 155 Loss: 1127.4034839576127\n",
      "Epoch: 156 Loss: 1125.609938124477\n",
      "Epoch: 157 Loss: 1123.824257094428\n",
      "Epoch: 158 Loss: 1122.0463947721928\n",
      "Epoch: 159 Loss: 1120.276305328046\n",
      "Epoch: 160 Loss: 1118.5139431965413\n",
      "Epoch: 161 Loss: 1116.7592630752908\n",
      "Epoch: 162 Loss: 1115.0122199236812\n",
      "Epoch: 163 Loss: 1113.2727689616856\n",
      "Epoch: 164 Loss: 1111.540865668584\n",
      "Epoch: 165 Loss: 1109.8164657817806\n",
      "Epoch: 166 Loss: 1108.0995252955124\n",
      "Epoch: 167 Loss: 1106.390000459709\n",
      "Epoch: 168 Loss: 1104.6878477786315\n",
      "Epoch: 169 Loss: 1102.9930240098306\n",
      "Epoch: 170 Loss: 1101.3054861627793\n",
      "Epoch: 171 Loss: 1099.625191497736\n",
      "Epoch: 172 Loss: 1097.9520975245118\n",
      "Epoch: 173 Loss: 1096.2861620012545\n",
      "Epoch: 174 Loss: 1094.6273429332937\n",
      "Epoch: 175 Loss: 1092.9755985718543\n",
      "Epoch: 176 Loss: 1091.330887412937\n",
      "Epoch: 177 Loss: 1089.693168196071\n",
      "Epoch: 178 Loss: 1088.0623999031602\n",
      "Epoch: 179 Loss: 1086.4385417572641\n",
      "Epoch: 180 Loss: 1084.8215532214335\n",
      "Epoch: 181 Loss: 1083.2113939975063\n",
      "Epoch: 182 Loss: 1081.6080240249564\n",
      "Epoch: 183 Loss: 1080.0114034797089\n",
      "Epoch: 184 Loss: 1078.4214927729304\n",
      "Epoch: 185 Loss: 1076.838252549925\n",
      "Epoch: 186 Loss: 1075.2616436889007\n",
      "Epoch: 187 Loss: 1073.6916272998938\n",
      "Epoch: 188 Loss: 1072.1281647234903\n",
      "Epoch: 189 Loss: 1070.5712175297722\n",
      "Epoch: 190 Loss: 1069.0207475171223\n",
      "Epoch: 191 Loss: 1067.4767167110747\n",
      "Epoch: 192 Loss: 1065.9390873631794\n",
      "Epoch: 193 Loss: 1064.4078219498365\n",
      "Epoch: 194 Loss: 1062.8828831711926\n",
      "Epoch: 195 Loss: 1061.364233949971\n",
      "Epoch: 196 Loss: 1059.8518374303558\n",
      "Epoch: 197 Loss: 1058.3456569768655\n",
      "Epoch: 198 Loss: 1056.845656173207\n",
      "Epoch: 199 Loss: 1055.351798821163\n",
      "Epoch: 200 Loss: 1053.8640489395063\n",
      "Epoch: 201 Loss: 1052.3823707628108\n",
      "Epoch: 202 Loss: 1050.9067287404114\n",
      "Epoch: 203 Loss: 1049.437087535245\n",
      "Epoch: 204 Loss: 1047.9734120227704\n",
      "Epoch: 205 Loss: 1046.5156672898747\n",
      "Epoch: 206 Loss: 1045.0638186337344\n",
      "Epoch: 207 Loss: 1043.6178315607588\n",
      "Epoch: 208 Loss: 1042.1776717854807\n",
      "Epoch: 209 Loss: 1040.7433052294773\n",
      "Epoch: 210 Loss: 1039.3146980202607\n",
      "Epoch: 211 Loss: 1037.8918164902557\n",
      "Epoch: 212 Loss: 1036.4746271756383\n",
      "Epoch: 213 Loss: 1035.0630968153243\n",
      "Epoch: 214 Loss: 1033.6571923498814\n",
      "Epoch: 215 Loss: 1032.2568809204608\n",
      "Epoch: 216 Loss: 1030.8621298677217\n",
      "Epoch: 217 Loss: 1029.4729067307887\n",
      "Epoch: 218 Loss: 1028.0891792461991\n",
      "Epoch: 219 Loss: 1026.7109153468232\n",
      "Epoch: 220 Loss: 1025.3380831608347\n",
      "Epoch: 221 Loss: 1023.9706510106789\n",
      "Epoch: 222 Loss: 1022.6085874119917\n",
      "Epoch: 223 Loss: 1021.251861072589\n",
      "Epoch: 224 Loss: 1019.9004408914309\n",
      "Epoch: 225 Loss: 1018.5542959575829\n",
      "Epoch: 226 Loss: 1017.2133955491865\n",
      "Epoch: 227 Loss: 1015.877709132448\n",
      "Epoch: 228 Loss: 1014.5472063605993\n",
      "Epoch: 229 Loss: 1013.2218570729057\n",
      "Epoch: 230 Loss: 1011.9016312936044\n",
      "Epoch: 231 Loss: 1010.5864992309869\n",
      "Epoch: 232 Loss: 1009.2764312762743\n",
      "Epoch: 233 Loss: 1007.9713980027198\n",
      "Epoch: 234 Loss: 1006.6713701645441\n",
      "Epoch: 235 Loss: 1005.3763186959743\n",
      "Epoch: 236 Loss: 1004.0862147102451\n",
      "Epoch: 237 Loss: 1002.8010294985921\n",
      "Epoch: 238 Loss: 1001.5207345293051\n",
      "Epoch: 239 Loss: 1000.2453014467095\n",
      "Epoch: 240 Loss: 998.9747020702333\n",
      "Epoch: 241 Loss: 997.7089083933779\n",
      "Epoch: 242 Loss: 996.44789258281\n",
      "Epoch: 243 Loss: 995.1916269773586\n",
      "Epoch: 244 Loss: 993.9400840870625\n",
      "Epoch: 245 Loss: 992.6932365921905\n",
      "Epoch: 246 Loss: 991.4510573423728\n",
      "Epoch: 247 Loss: 990.2135193555173\n",
      "Epoch: 248 Loss: 988.9805958169896\n",
      "Epoch: 249 Loss: 987.7522600785835\n",
      "Epoch: 250 Loss: 986.5284856576293\n",
      "Epoch: 251 Loss: 985.3092462360372\n",
      "Epoch: 252 Loss: 984.0945156593772\n",
      "Epoch: 253 Loss: 982.884267935949\n",
      "Epoch: 254 Loss: 981.6784772358561\n",
      "Epoch: 255 Loss: 980.4771178900708\n",
      "Epoch: 256 Loss: 979.2801643895687\n",
      "Epoch: 257 Loss: 978.0875913843329\n",
      "Epoch: 258 Loss: 976.8993736825412\n",
      "Epoch: 259 Loss: 975.715486249584\n",
      "Epoch: 260 Loss: 974.5359042071913\n",
      "Epoch: 261 Loss: 973.3606028325601\n",
      "Epoch: 262 Loss: 972.1895575574113\n",
      "Epoch: 263 Loss: 971.0227439671438\n",
      "Epoch: 264 Loss: 969.8601377999126\n",
      "Epoch: 265 Loss: 968.7017149457773\n",
      "Epoch: 266 Loss: 967.5474514458168\n",
      "Epoch: 267 Loss: 966.397323491222\n",
      "Epoch: 268 Loss: 965.2513074224888\n",
      "Epoch: 269 Loss: 964.1093797284747\n",
      "Epoch: 270 Loss: 962.971517045586\n",
      "Epoch: 271 Loss: 961.8376961569189\n",
      "Epoch: 272 Loss: 960.7078939913828\n",
      "Epoch: 273 Loss: 959.5820876228419\n",
      "Epoch: 274 Loss: 958.4602542692935\n",
      "Epoch: 275 Loss: 957.342371292019\n",
      "Epoch: 276 Loss: 956.2284161947065\n",
      "Epoch: 277 Loss: 955.1183666226758\n",
      "Epoch: 278 Loss: 954.0122003619827\n",
      "Epoch: 279 Loss: 952.90989533862\n",
      "Epoch: 280 Loss: 951.8114296177026\n",
      "Epoch: 281 Loss: 950.7167814026285\n",
      "Epoch: 282 Loss: 949.6259290342504\n",
      "Epoch: 283 Loss: 948.5388509900878\n",
      "Epoch: 284 Loss: 947.4555258834857\n",
      "Epoch: 285 Loss: 946.3759324628427\n",
      "Epoch: 286 Loss: 945.3000496107795\n",
      "Epoch: 287 Loss: 944.227856343359\n",
      "Epoch: 288 Loss: 943.1593318092484\n",
      "Epoch: 289 Loss: 942.0944552889977\n",
      "Epoch: 290 Loss: 941.0332061941873\n",
      "Epoch: 291 Loss: 939.9755640666609\n",
      "Epoch: 292 Loss: 938.9215085777845\n",
      "Epoch: 293 Loss: 937.8710195275951\n",
      "Epoch: 294 Loss: 936.8240768440922\n",
      "Epoch: 295 Loss: 935.7806605824358\n",
      "Epoch: 296 Loss: 934.7407509241815\n",
      "Epoch: 297 Loss: 933.704328176532\n",
      "Epoch: 298 Loss: 932.6713727715523\n",
      "Epoch: 299 Loss: 931.6418652654652\n",
      "Epoch: 300 Loss: 930.6157863378214\n",
      "Epoch: 301 Loss: 929.5931167908248\n",
      "Epoch: 302 Loss: 928.5738375485513\n",
      "Epoch: 303 Loss: 927.5579296562048\n",
      "Epoch: 304 Loss: 926.5453742794098\n",
      "Epoch: 305 Loss: 925.5361527034527\n",
      "Epoch: 306 Loss: 924.5302463325428\n",
      "Epoch: 307 Loss: 923.5276366891158\n",
      "Epoch: 308 Loss: 922.528305413098\n",
      "Epoch: 309 Loss: 921.5322342611684\n",
      "Epoch: 310 Loss: 920.5394051060867\n",
      "Epoch: 311 Loss: 919.5497999359332\n",
      "Epoch: 312 Loss: 918.5634008534481\n",
      "Epoch: 313 Loss: 917.58019007526\n",
      "Epoch: 314 Loss: 916.6001499312649\n",
      "Epoch: 315 Loss: 915.6232628638736\n",
      "Epoch: 316 Loss: 914.6495114273413\n",
      "Epoch: 317 Loss: 913.6788782870598\n",
      "Epoch: 318 Loss: 912.7113462188973\n",
      "Epoch: 319 Loss: 911.746898108472\n",
      "Epoch: 320 Loss: 910.7855169505342\n",
      "Epoch: 321 Loss: 909.8271858482182\n",
      "Epoch: 322 Loss: 908.8718880124376\n",
      "Epoch: 323 Loss: 907.9196067611565\n",
      "Epoch: 324 Loss: 906.9703255187735\n",
      "Epoch: 325 Loss: 906.0240278154308\n",
      "Epoch: 326 Loss: 905.0806972863542\n",
      "Epoch: 327 Loss: 904.1403176712164\n",
      "Epoch: 328 Loss: 903.20287281346\n",
      "Epoch: 329 Loss: 902.2683466596833\n",
      "Epoch: 330 Loss: 901.3367232589521\n",
      "Epoch: 331 Loss: 900.4079867622057\n",
      "Epoch: 332 Loss: 899.4821214215688\n",
      "Epoch: 333 Loss: 898.5591115897572\n",
      "Epoch: 334 Loss: 897.6389417194213\n",
      "Epoch: 335 Loss: 896.7215963625431\n",
      "Epoch: 336 Loss: 895.8070601697733\n",
      "Epoch: 337 Loss: 894.8953178898513\n",
      "Epoch: 338 Loss: 893.9863543689713\n",
      "Epoch: 339 Loss: 893.0801545501486\n",
      "Epoch: 340 Loss: 892.1767034726254\n",
      "Epoch: 341 Loss: 891.2759862712833\n",
      "Epoch: 342 Loss: 890.3779881759924\n",
      "Epoch: 343 Loss: 889.4826945110491\n",
      "Epoch: 344 Loss: 888.5900906945624\n",
      "Epoch: 345 Loss: 887.7001622378684\n",
      "Epoch: 346 Loss: 886.8128947449179\n",
      "Epoch: 347 Loss: 885.9282739117062\n",
      "Epoch: 348 Loss: 885.0462855256822\n",
      "Epoch: 349 Loss: 884.1669154651865\n",
      "Epoch: 350 Loss: 883.2901496988217\n",
      "Epoch: 351 Loss: 882.4159742849373\n",
      "Epoch: 352 Loss: 881.5443753710226\n",
      "Epoch: 353 Loss: 880.6753391931396\n",
      "Epoch: 354 Loss: 879.8088520753834\n",
      "Epoch: 355 Loss: 878.9449004292716\n",
      "Epoch: 356 Loss: 878.0834707532364\n",
      "Epoch: 357 Loss: 877.224549632038\n",
      "Epoch: 358 Loss: 876.3681237362091\n",
      "Epoch: 359 Loss: 875.5141798215369\n",
      "Epoch: 360 Loss: 874.6627047284838\n",
      "Epoch: 361 Loss: 873.8136853816491\n",
      "Epoch: 362 Loss: 872.967108789239\n",
      "Epoch: 363 Loss: 872.1229620425249\n",
      "Epoch: 364 Loss: 871.2812323153167\n",
      "Epoch: 365 Loss: 870.4419068634024\n",
      "Epoch: 366 Loss: 869.6049730240694\n",
      "Epoch: 367 Loss: 868.7704182155237\n",
      "Epoch: 368 Loss: 867.9382299364158\n",
      "Epoch: 369 Loss: 867.1083957652866\n",
      "Epoch: 370 Loss: 866.2809033600668\n",
      "Epoch: 371 Loss: 865.4557404575735\n",
      "Epoch: 372 Loss: 864.632894872981\n",
      "Epoch: 373 Loss: 863.8123544993217\n",
      "Epoch: 374 Loss: 862.9941073069825\n",
      "Epoch: 375 Loss: 862.1781413432035\n",
      "Epoch: 376 Loss: 861.3644447315836\n",
      "Epoch: 377 Loss: 860.5530056715728\n",
      "Epoch: 378 Loss: 859.7438124379825\n",
      "Epoch: 379 Loss: 858.9368533805238\n",
      "Epoch: 380 Loss: 858.1321169232792\n",
      "Epoch: 381 Loss: 857.3295915642373\n",
      "Epoch: 382 Loss: 856.5292658748223\n",
      "Epoch: 383 Loss: 855.7311284993939\n",
      "Epoch: 384 Loss: 854.9351681547993\n",
      "Epoch: 385 Loss: 854.1413736298776\n",
      "Epoch: 386 Loss: 853.3497337849899\n",
      "Epoch: 387 Loss: 852.5602375515801\n",
      "Epoch: 388 Loss: 851.7728739316644\n",
      "Epoch: 389 Loss: 850.9876319974246\n",
      "Epoch: 390 Loss: 850.2045008907032\n",
      "Epoch: 391 Loss: 849.423469822583\n",
      "Epoch: 392 Loss: 848.6445280729034\n",
      "Epoch: 393 Loss: 847.8676649898235\n",
      "Epoch: 394 Loss: 847.0928699893893\n",
      "Epoch: 395 Loss: 846.32013255506\n",
      "Epoch: 396 Loss: 845.5494422373142\n",
      "Epoch: 397 Loss: 844.7807886531282\n",
      "Epoch: 398 Loss: 844.0141614856191\n",
      "Epoch: 399 Loss: 843.2495504835704\n",
      "Epoch: 400 Loss: 842.4869454610208\n",
      "Epoch: 401 Loss: 841.7263362968148\n",
      "Epoch: 402 Loss: 840.9677129341962\n",
      "Epoch: 403 Loss: 840.2110653803646\n",
      "Epoch: 404 Loss: 839.4563837060678\n",
      "Epoch: 405 Loss: 838.7036580451745\n",
      "Epoch: 406 Loss: 837.9528785942824\n",
      "Epoch: 407 Loss: 837.204035612277\n",
      "Epoch: 408 Loss: 836.4571194199215\n",
      "Epoch: 409 Loss: 835.7121203994741\n",
      "Epoch: 410 Loss: 834.9690289942407\n",
      "Epoch: 411 Loss: 834.2278357082322\n",
      "Epoch: 412 Loss: 833.4885311056994\n",
      "Epoch: 413 Loss: 832.7511058107854\n",
      "Epoch: 414 Loss: 832.0155505070852\n",
      "Epoch: 415 Loss: 831.2818559372976\n",
      "Epoch: 416 Loss: 830.5500129028057\n",
      "Epoch: 417 Loss: 829.8200122632863\n",
      "Epoch: 418 Loss: 829.091844936347\n",
      "Epoch: 419 Loss: 828.3655018971269\n",
      "Epoch: 420 Loss: 827.6409741779016\n",
      "Epoch: 421 Loss: 826.9182528677454\n",
      "Epoch: 422 Loss: 826.1973291121047\n",
      "Epoch: 423 Loss: 825.4781941124735\n",
      "Epoch: 424 Loss: 824.760839125977\n",
      "Epoch: 425 Loss: 824.0452554650377\n",
      "Epoch: 426 Loss: 823.3314344969793\n",
      "Epoch: 427 Loss: 822.6193676436964\n",
      "Epoch: 428 Loss: 821.909046381249\n",
      "Epoch: 429 Loss: 821.2004622395552\n",
      "Epoch: 430 Loss: 820.4936068019599\n",
      "Epoch: 431 Loss: 819.788471704971\n",
      "Epoch: 432 Loss: 819.0850486378321\n",
      "Epoch: 433 Loss: 818.3833293422019\n",
      "Epoch: 434 Loss: 817.6833056117927\n",
      "Epoch: 435 Loss: 816.9849692920599\n",
      "Epoch: 436 Loss: 816.288312279801\n",
      "Epoch: 437 Loss: 815.5933265228606\n",
      "Epoch: 438 Loss: 814.9000040197669\n",
      "Epoch: 439 Loss: 814.2083368193945\n",
      "Epoch: 440 Loss: 813.5183170206492\n",
      "Epoch: 441 Loss: 812.8299367721085\n",
      "Epoch: 442 Loss: 812.1431882716956\n",
      "Epoch: 443 Loss: 811.4580637663773\n",
      "Epoch: 444 Loss: 810.7745555517986\n",
      "Epoch: 445 Loss: 810.0926559719742\n",
      "Epoch: 446 Loss: 809.4123574189762\n",
      "Epoch: 447 Loss: 808.7336523325843\n",
      "Epoch: 448 Loss: 808.0565332000227\n",
      "Epoch: 449 Loss: 807.3809925555682\n",
      "Epoch: 450 Loss: 806.7070229803033\n",
      "Epoch: 451 Loss: 806.0346171017513\n",
      "Epoch: 452 Loss: 805.3637675936092\n",
      "Epoch: 453 Loss: 804.6944671754077\n",
      "Epoch: 454 Loss: 804.0267086122187\n",
      "Epoch: 455 Loss: 803.3604847143546\n",
      "Epoch: 456 Loss: 802.6957883370484\n",
      "Epoch: 457 Loss: 802.032612380178\n",
      "Epoch: 458 Loss: 801.370949787942\n",
      "Epoch: 459 Loss: 800.7107935485838\n",
      "Epoch: 460 Loss: 800.0521366940756\n",
      "Epoch: 461 Loss: 799.3949722998591\n",
      "Epoch: 462 Loss: 798.7392934845152\n",
      "Epoch: 463 Loss: 798.0850934094994\n",
      "Epoch: 464 Loss: 797.4323652788507\n",
      "Epoch: 465 Loss: 796.7811023389064\n",
      "Epoch: 466 Loss: 796.1312978780218\n",
      "Epoch: 467 Loss: 795.4829452262569\n",
      "Epoch: 468 Loss: 794.8360377551635\n",
      "Epoch: 469 Loss: 794.1905688774405\n",
      "Epoch: 470 Loss: 793.5465320466893\n",
      "Epoch: 471 Loss: 792.9039207571469\n",
      "Epoch: 472 Loss: 792.2627285433941\n",
      "Epoch: 473 Loss: 791.6229489800905\n",
      "Epoch: 474 Loss: 790.9845756817169\n",
      "Epoch: 475 Loss: 790.347602302291\n",
      "Epoch: 476 Loss: 789.7120225351222\n",
      "Epoch: 477 Loss: 789.0778301125301\n",
      "Epoch: 478 Loss: 788.4450188055897\n",
      "Epoch: 479 Loss: 787.8135824238786\n",
      "Epoch: 480 Loss: 787.1835148151977\n",
      "Epoch: 481 Loss: 786.5548098653488\n",
      "Epoch: 482 Loss: 785.9274614978415\n",
      "Epoch: 483 Loss: 785.3014636736714\n",
      "Epoch: 484 Loss: 784.6768103910485\n",
      "Epoch: 485 Loss: 784.0534956851579\n",
      "Epoch: 486 Loss: 783.4315136279073\n",
      "Epoch: 487 Loss: 782.8108583276913\n",
      "Epoch: 488 Loss: 782.1915239291242\n",
      "Epoch: 489 Loss: 781.573504612809\n",
      "Epoch: 490 Loss: 780.9567945951055\n",
      "Epoch: 491 Loss: 780.3413881278848\n",
      "Epoch: 492 Loss: 779.7272794982699\n",
      "Epoch: 493 Loss: 779.1144630284407\n",
      "Epoch: 494 Loss: 778.5029330753547\n",
      "Epoch: 495 Loss: 777.8926840305495\n",
      "Epoch: 496 Loss: 777.2837103198823\n",
      "Epoch: 497 Loss: 776.6760064033289\n",
      "Epoch: 498 Loss: 776.0695667747134\n",
      "Epoch: 499 Loss: 775.464385961522\n",
      "Epoch: 500 Loss: 774.8604585246708\n",
      "Epoch: 501 Loss: 774.2577790582507\n",
      "Epoch: 502 Loss: 773.6563421893292\n",
      "Epoch: 503 Loss: 773.0561425777465\n",
      "Epoch: 504 Loss: 772.4571749158458\n",
      "Epoch: 505 Loss: 771.8594339283054\n",
      "Epoch: 506 Loss: 771.2629143718884\n",
      "Epoch: 507 Loss: 770.6676110352441\n",
      "Epoch: 508 Loss: 770.0735187386755\n",
      "Epoch: 509 Loss: 769.4806323339717\n",
      "Epoch: 510 Loss: 768.8889467041399\n",
      "Epoch: 511 Loss: 768.2984567632184\n",
      "Epoch: 512 Loss: 767.7091574560991\n",
      "Epoch: 513 Loss: 767.1210437582572\n",
      "Epoch: 514 Loss: 766.5341106756132\n",
      "Epoch: 515 Loss: 765.9483532442757\n",
      "Epoch: 516 Loss: 765.363766530377\n",
      "Epoch: 517 Loss: 764.7803456298387\n",
      "Epoch: 518 Loss: 764.1980856682069\n",
      "Epoch: 519 Loss: 763.616981800413\n",
      "Epoch: 520 Loss: 763.0370292106162\n",
      "Epoch: 521 Loss: 762.4582231119856\n",
      "Epoch: 522 Loss: 761.8805587465029\n",
      "Epoch: 523 Loss: 761.3040313847935\n",
      "Epoch: 524 Loss: 760.7286363258944\n",
      "Epoch: 525 Loss: 760.1543688971201\n",
      "Epoch: 526 Loss: 759.5812244538004\n",
      "Epoch: 527 Loss: 759.0091983791565\n",
      "Epoch: 528 Loss: 758.4382860840836\n",
      "Epoch: 529 Loss: 757.8684830069728\n",
      "Epoch: 530 Loss: 757.29978461353\n",
      "Epoch: 531 Loss: 756.7321863965833\n",
      "Epoch: 532 Loss: 756.1656838759059\n",
      "Epoch: 533 Loss: 755.6002725980403\n",
      "Epoch: 534 Loss: 755.0359481361191\n",
      "Epoch: 535 Loss: 754.4727060896696\n",
      "Epoch: 536 Loss: 753.9105420844695\n",
      "Epoch: 537 Loss: 753.3494517723366\n",
      "Epoch: 538 Loss: 752.7894308309774\n",
      "Epoch: 539 Loss: 752.2304749638084\n",
      "Epoch: 540 Loss: 751.672579899781\n",
      "Epoch: 541 Loss: 751.1157413932156\n",
      "Epoch: 542 Loss: 750.5599552236221\n",
      "Epoch: 543 Loss: 750.0052171955438\n",
      "Epoch: 544 Loss: 749.451523138382\n",
      "Epoch: 545 Loss: 748.8988689062354\n",
      "Epoch: 546 Loss: 748.3472503777259\n",
      "Epoch: 547 Loss: 747.7966634558508\n",
      "Epoch: 548 Loss: 747.2471040677991\n",
      "Epoch: 549 Loss: 746.6985681648076\n",
      "Epoch: 550 Loss: 746.1510517219933\n",
      "Epoch: 551 Loss: 745.6045507381878\n",
      "Epoch: 552 Loss: 745.0590612357903\n",
      "Epoch: 553 Loss: 744.5145792606165\n",
      "Epoch: 554 Loss: 743.9711008817035\n",
      "Epoch: 555 Loss: 743.4286221912176\n",
      "Epoch: 556 Loss: 742.8871393042291\n",
      "Epoch: 557 Loss: 742.3466483586227\n",
      "Epoch: 558 Loss: 741.8071455149085\n",
      "Epoch: 559 Loss: 741.2686269560812\n",
      "Epoch: 560 Loss: 740.7310888874647\n",
      "Epoch: 561 Loss: 740.1945275365781\n",
      "Epoch: 562 Loss: 739.6589391529706\n",
      "Epoch: 563 Loss: 739.1243200080892\n",
      "Epoch: 564 Loss: 738.5906663951203\n",
      "Epoch: 565 Loss: 738.0579746288619\n",
      "Epoch: 566 Loss: 737.5262410455433\n",
      "Epoch: 567 Loss: 736.9954620027394\n",
      "Epoch: 568 Loss: 736.4656338791707\n",
      "Epoch: 569 Loss: 735.9367530746096\n",
      "Epoch: 570 Loss: 735.4088160097081\n",
      "Epoch: 571 Loss: 734.881819125883\n",
      "Epoch: 572 Loss: 734.355758885142\n",
      "Epoch: 573 Loss: 733.8306317699947\n",
      "Epoch: 574 Loss: 733.306434283275\n",
      "Epoch: 575 Loss: 732.7831629480448\n",
      "Epoch: 576 Loss: 732.2608143074177\n",
      "Epoch: 577 Loss: 731.7393849244552\n",
      "Epoch: 578 Loss: 731.2188713820294\n",
      "Epoch: 579 Loss: 730.6992702826731\n",
      "Epoch: 580 Loss: 730.1805782484905\n",
      "Epoch: 581 Loss: 729.662791920972\n",
      "Epoch: 582 Loss: 729.1459079609161\n",
      "Epoch: 583 Loss: 728.6299230482691\n",
      "Epoch: 584 Loss: 728.1148338820229\n",
      "Epoch: 585 Loss: 727.600637180056\n",
      "Epoch: 586 Loss: 727.0873296790477\n",
      "Epoch: 587 Loss: 726.5749081342996\n",
      "Epoch: 588 Loss: 726.0633693197037\n",
      "Epoch: 589 Loss: 725.5527100275137\n",
      "Epoch: 590 Loss: 725.0429270682885\n",
      "Epoch: 591 Loss: 724.5340172707638\n",
      "Epoch: 592 Loss: 724.0259774816991\n",
      "Epoch: 593 Loss: 723.5188045658148\n",
      "Epoch: 594 Loss: 723.0124954056162\n",
      "Epoch: 595 Loss: 722.5070469013056\n",
      "Epoch: 596 Loss: 722.0024559706832\n",
      "Epoch: 597 Loss: 721.4987195489762\n",
      "Epoch: 598 Loss: 720.9958345887716\n",
      "Epoch: 599 Loss: 720.4937980598947\n",
      "Epoch: 600 Loss: 719.992606949279\n",
      "Epoch: 601 Loss: 719.4922582608641\n",
      "Epoch: 602 Loss: 718.992749015477\n",
      "Epoch: 603 Loss: 718.4940762507372\n",
      "Epoch: 604 Loss: 717.9962370209232\n",
      "Epoch: 605 Loss: 717.4992283968718\n",
      "Epoch: 606 Loss: 717.0030474658882\n",
      "Epoch: 607 Loss: 716.5076913316054\n",
      "Epoch: 608 Loss: 716.0131571138957\n",
      "Epoch: 609 Loss: 715.5194419487732\n",
      "Epoch: 610 Loss: 715.0265429882579\n",
      "Epoch: 611 Loss: 714.5344574002975\n",
      "Epoch: 612 Loss: 714.0431823686523\n",
      "Epoch: 613 Loss: 713.5527150927918\n",
      "Epoch: 614 Loss: 713.0630527877959\n",
      "Epoch: 615 Loss: 712.5741926842483\n",
      "Epoch: 616 Loss: 712.0861320281219\n",
      "Epoch: 617 Loss: 711.598868080714\n",
      "Epoch: 618 Loss: 711.1123981185062\n",
      "Epoch: 619 Loss: 710.6267194330992\n",
      "Epoch: 620 Loss: 710.1418293310857\n",
      "Epoch: 621 Loss: 709.6577251339512\n",
      "Epoch: 622 Loss: 709.1744041780286\n",
      "Epoch: 623 Loss: 708.6918638143197\n",
      "Epoch: 624 Loss: 708.210101408468\n",
      "Epoch: 625 Loss: 707.7291143406202\n",
      "Epoch: 626 Loss: 707.248900005357\n",
      "Epoch: 627 Loss: 706.7694558115905\n",
      "Epoch: 628 Loss: 706.2907791824614\n",
      "Epoch: 629 Loss: 705.8128675552691\n",
      "Epoch: 630 Loss: 705.3357183813504\n",
      "Epoch: 631 Loss: 704.8593291260072\n",
      "Epoch: 632 Loss: 704.3836972684152\n",
      "Epoch: 633 Loss: 703.908820301538\n",
      "Epoch: 634 Loss: 703.4346957319959\n",
      "Epoch: 635 Loss: 702.9613210800449\n",
      "Epoch: 636 Loss: 702.488693879436\n",
      "Epoch: 637 Loss: 702.0168116773488\n",
      "Epoch: 638 Loss: 701.5456720342964\n",
      "Epoch: 639 Loss: 701.0752725240569\n",
      "Epoch: 640 Loss: 700.6056107335536\n",
      "Epoch: 641 Loss: 700.1366842627817\n",
      "Epoch: 642 Loss: 699.6684907247702\n",
      "Epoch: 643 Loss: 699.2010277454254\n",
      "Epoch: 644 Loss: 698.7342929634809\n",
      "Epoch: 645 Loss: 698.2682840304259\n",
      "Epoch: 646 Loss: 697.8029986104051\n",
      "Epoch: 647 Loss: 697.3384343801418\n",
      "Epoch: 648 Loss: 696.8745890288561\n",
      "Epoch: 649 Loss: 696.4114602581915\n",
      "Epoch: 650 Loss: 695.9490457821117\n",
      "Epoch: 651 Loss: 695.4873433268567\n",
      "Epoch: 652 Loss: 695.0263506308307\n",
      "Epoch: 653 Loss: 694.5660654445454\n",
      "Epoch: 654 Loss: 694.1064855305212\n",
      "Epoch: 655 Loss: 693.6476086632439\n",
      "Epoch: 656 Loss: 693.1894326290341\n",
      "Epoch: 657 Loss: 692.7319552260336\n",
      "Epoch: 658 Loss: 692.2751742640844\n",
      "Epoch: 659 Loss: 691.8190875646624\n",
      "Epoch: 660 Loss: 691.3636929608151\n",
      "Epoch: 661 Loss: 690.9089882970779\n",
      "Epoch: 662 Loss: 690.454971429399\n",
      "Epoch: 663 Loss: 690.0016402250826\n",
      "Epoch: 664 Loss: 689.5489925626739\n",
      "Epoch: 665 Loss: 689.0970263319468\n",
      "Epoch: 666 Loss: 688.645739433779\n",
      "Epoch: 667 Loss: 688.1951297801155\n",
      "Epoch: 668 Loss: 687.7451952938709\n",
      "Epoch: 669 Loss: 687.2959339088819\n",
      "Epoch: 670 Loss: 686.8473435698471\n",
      "Epoch: 671 Loss: 686.3994222321936\n",
      "Epoch: 672 Loss: 685.9521678620866\n",
      "Epoch: 673 Loss: 685.5055784363304\n",
      "Epoch: 674 Loss: 685.0596519422767\n",
      "Epoch: 675 Loss: 684.6143863777809\n",
      "Epoch: 676 Loss: 684.1697797511596\n",
      "Epoch: 677 Loss: 683.7258300810641\n",
      "Epoch: 678 Loss: 683.282535396459\n",
      "Epoch: 679 Loss: 682.8398937365572\n",
      "Epoch: 680 Loss: 682.3979031507229\n",
      "Epoch: 681 Loss: 681.9565616984365\n",
      "Epoch: 682 Loss: 681.5158674492183\n",
      "Epoch: 683 Loss: 681.075818482561\n",
      "Epoch: 684 Loss: 680.6364128878777\n",
      "Epoch: 685 Loss: 680.1976487644318\n",
      "Epoch: 686 Loss: 679.7595242212717\n",
      "Epoch: 687 Loss: 679.322037377179\n",
      "Epoch: 688 Loss: 678.8851863605943\n",
      "Epoch: 689 Loss: 678.4489693095618\n",
      "Epoch: 690 Loss: 678.0133843716723\n",
      "Epoch: 691 Loss: 677.5784297039996\n",
      "Epoch: 692 Loss: 677.1441034730366\n",
      "Epoch: 693 Loss: 676.7104038546387\n",
      "Epoch: 694 Loss: 676.277329033965\n",
      "Epoch: 695 Loss: 675.844877205417\n",
      "Epoch: 696 Loss: 675.4130465725976\n",
      "Epoch: 697 Loss: 674.9818353482198\n",
      "Epoch: 698 Loss: 674.5512417540679\n",
      "Epoch: 699 Loss: 674.1212640209567\n",
      "Epoch: 700 Loss: 673.6919003886424\n",
      "Epoch: 701 Loss: 673.2631491057886\n",
      "Epoch: 702 Loss: 672.8350084299078\n",
      "Epoch: 703 Loss: 672.4074766272847\n",
      "Epoch: 704 Loss: 671.9805519729586\n",
      "Epoch: 705 Loss: 671.5542327506263\n",
      "Epoch: 706 Loss: 671.1285172526278\n",
      "Epoch: 707 Loss: 670.7034037798663\n",
      "Epoch: 708 Loss: 670.2788906417529\n",
      "Epoch: 709 Loss: 669.8549761561814\n",
      "Epoch: 710 Loss: 669.431658649436\n",
      "Epoch: 711 Loss: 669.0089364561628\n",
      "Epoch: 712 Loss: 668.5868079193277\n",
      "Epoch: 713 Loss: 668.1652713901258\n",
      "Epoch: 714 Loss: 667.7443252279684\n",
      "Epoch: 715 Loss: 667.323967800427\n",
      "Epoch: 716 Loss: 666.9041974831473\n",
      "Epoch: 717 Loss: 666.4850126598413\n",
      "Epoch: 718 Loss: 666.0664117222193\n",
      "Epoch: 719 Loss: 665.648393069929\n",
      "Epoch: 720 Loss: 665.2309551105305\n",
      "Epoch: 721 Loss: 664.8140962594317\n",
      "Epoch: 722 Loss: 664.3978149398338\n",
      "Epoch: 723 Loss: 663.9821095826991\n",
      "Epoch: 724 Loss: 663.5669786266905\n",
      "Epoch: 725 Loss: 663.1524205181341\n",
      "Epoch: 726 Loss: 662.7384337109538\n",
      "Epoch: 727 Loss: 662.3250166666484\n",
      "Epoch: 728 Loss: 661.9121678542177\n",
      "Epoch: 729 Loss: 661.4998857501477\n",
      "Epoch: 730 Loss: 661.0881688383331\n",
      "Epoch: 731 Loss: 660.6770156100499\n",
      "Epoch: 732 Loss: 660.2664245639131\n",
      "Epoch: 733 Loss: 659.8563942057998\n",
      "Epoch: 734 Loss: 659.4469230488633\n",
      "Epoch: 735 Loss: 659.0380096134168\n",
      "Epoch: 736 Loss: 658.6296524269555\n",
      "Epoch: 737 Loss: 658.2218500240516\n",
      "Epoch: 738 Loss: 657.8146009463755\n",
      "Epoch: 739 Loss: 657.407903742598\n",
      "Epoch: 740 Loss: 657.0017569683604\n",
      "Epoch: 741 Loss: 656.5961591862542\n",
      "Epoch: 742 Loss: 656.1911089657569\n",
      "Epoch: 743 Loss: 655.7866048831916\n",
      "Epoch: 744 Loss: 655.3826455216894\n",
      "Epoch: 745 Loss: 654.9792294711548\n",
      "Epoch: 746 Loss: 654.5763553282108\n",
      "Epoch: 747 Loss: 654.1740216961596\n",
      "Epoch: 748 Loss: 653.7722271849567\n",
      "Epoch: 749 Loss: 653.3709704111408\n",
      "Epoch: 750 Loss: 652.9702499978325\n",
      "Epoch: 751 Loss: 652.570064574657\n",
      "Epoch: 752 Loss: 652.1704127777401\n",
      "Epoch: 753 Loss: 651.7712932496215\n",
      "Epoch: 754 Loss: 651.3727046392651\n",
      "Epoch: 755 Loss: 650.974645601995\n",
      "Epoch: 756 Loss: 650.5771147994465\n",
      "Epoch: 757 Loss: 650.1801108995629\n",
      "Epoch: 758 Loss: 649.7836325765179\n",
      "Epoch: 759 Loss: 649.3876785106999\n",
      "Epoch: 760 Loss: 648.9922473886775\n",
      "Epoch: 761 Loss: 648.5973379031446\n",
      "Epoch: 762 Loss: 648.2029487528908\n",
      "Epoch: 763 Loss: 647.809078642779\n",
      "Epoch: 764 Loss: 647.4157262836866\n",
      "Epoch: 765 Loss: 647.0228903924692\n",
      "Epoch: 766 Loss: 646.6305696919641\n",
      "Epoch: 767 Loss: 646.238762910894\n",
      "Epoch: 768 Loss: 645.8474687838757\n",
      "Epoch: 769 Loss: 645.4566860513647\n",
      "Epoch: 770 Loss: 645.0664134596238\n",
      "Epoch: 771 Loss: 644.6766497607077\n",
      "Epoch: 772 Loss: 644.287393712382\n",
      "Epoch: 773 Loss: 643.8986440781317\n",
      "Epoch: 774 Loss: 643.5103996271199\n",
      "Epoch: 775 Loss: 643.1226591341323\n",
      "Epoch: 776 Loss: 642.7354213795617\n",
      "Epoch: 777 Loss: 642.3486851493682\n",
      "Epoch: 778 Loss: 641.9624492350518\n",
      "Epoch: 779 Loss: 641.5767124336165\n",
      "Epoch: 780 Loss: 641.1914735475265\n",
      "Epoch: 781 Loss: 640.8067313846803\n",
      "Epoch: 782 Loss: 640.4224847583988\n",
      "Epoch: 783 Loss: 640.0387324873601\n",
      "Epoch: 784 Loss: 639.6554733955809\n",
      "Epoch: 785 Loss: 639.2727063123876\n",
      "Epoch: 786 Loss: 638.8904300723917\n",
      "Epoch: 787 Loss: 638.5086435154293\n",
      "Epoch: 788 Loss: 638.127345486578\n",
      "Epoch: 789 Loss: 637.7465348360802\n",
      "Epoch: 790 Loss: 637.3662104193213\n",
      "Epoch: 791 Loss: 636.9863710968249\n",
      "Epoch: 792 Loss: 636.6070157342002\n",
      "Epoch: 793 Loss: 636.2281432021185\n",
      "Epoch: 794 Loss: 635.8497523762611\n",
      "Epoch: 795 Loss: 635.471842137352\n",
      "Epoch: 796 Loss: 635.0944113710376\n",
      "Epoch: 797 Loss: 634.7174589679436\n",
      "Epoch: 798 Loss: 634.3409838235898\n",
      "Epoch: 799 Loss: 633.9649848383772\n",
      "Epoch: 800 Loss: 633.5894609175754\n",
      "Epoch: 801 Loss: 633.2144109712625\n",
      "Epoch: 802 Loss: 632.8398339143301\n",
      "Epoch: 803 Loss: 632.4657286664224\n",
      "Epoch: 804 Loss: 632.092094151947\n",
      "Epoch: 805 Loss: 631.7189292999955\n",
      "Epoch: 806 Loss: 631.3462330443765\n",
      "Epoch: 807 Loss: 630.9740043235215\n",
      "Epoch: 808 Loss: 630.6022420805223\n",
      "Epoch: 809 Loss: 630.2309452630684\n",
      "Epoch: 810 Loss: 629.8601128234125\n",
      "Epoch: 811 Loss: 629.4897437183605\n",
      "Epoch: 812 Loss: 629.119836909253\n",
      "Epoch: 813 Loss: 628.7503913619172\n",
      "Epoch: 814 Loss: 628.3814060466508\n",
      "Epoch: 815 Loss: 628.0128799382019\n",
      "Epoch: 816 Loss: 627.6448120157304\n",
      "Epoch: 817 Loss: 627.2772012627879\n",
      "Epoch: 818 Loss: 626.910046667297\n",
      "Epoch: 819 Loss: 626.5433472215243\n",
      "Epoch: 820 Loss: 626.1771019220488\n",
      "Epoch: 821 Loss: 625.8113097697286\n",
      "Epoch: 822 Loss: 625.4459697697114\n",
      "Epoch: 823 Loss: 625.0810809313749\n",
      "Epoch: 824 Loss: 624.7166422683046\n",
      "Epoch: 825 Loss: 624.3526527982932\n",
      "Epoch: 826 Loss: 623.9891115433007\n",
      "Epoch: 827 Loss: 623.6260175294068\n",
      "Epoch: 828 Loss: 623.2633697868454\n",
      "Epoch: 829 Loss: 622.9011673499309\n",
      "Epoch: 830 Loss: 622.539409257047\n",
      "Epoch: 831 Loss: 622.1780945506283\n",
      "Epoch: 832 Loss: 621.8172222771384\n",
      "Epoch: 833 Loss: 621.4567914870423\n",
      "Epoch: 834 Loss: 621.0968012347862\n",
      "Epoch: 835 Loss: 620.7372505787689\n",
      "Epoch: 836 Loss: 620.3781385813237\n",
      "Epoch: 837 Loss: 620.0194643086937\n",
      "Epoch: 838 Loss: 619.661226831021\n",
      "Epoch: 839 Loss: 619.3034252222957\n",
      "Epoch: 840 Loss: 618.9460585603764\n",
      "Epoch: 841 Loss: 618.5891259269243\n",
      "Epoch: 842 Loss: 618.2326264074089\n",
      "Epoch: 843 Loss: 617.8765590910756\n",
      "Epoch: 844 Loss: 617.520923070934\n",
      "Epoch: 845 Loss: 617.1657174437154\n",
      "Epoch: 846 Loss: 616.8109413098873\n",
      "Epoch: 847 Loss: 616.456593773588\n",
      "Epoch: 848 Loss: 616.1026739426402\n",
      "Epoch: 849 Loss: 615.7491809285185\n",
      "Epoch: 850 Loss: 615.3961138463278\n",
      "Epoch: 851 Loss: 615.0434718147716\n",
      "Epoch: 852 Loss: 614.6912539561633\n",
      "Epoch: 853 Loss: 614.3394593963649\n",
      "Epoch: 854 Loss: 613.988087264807\n",
      "Epoch: 855 Loss: 613.6371366944322\n",
      "Epoch: 856 Loss: 613.2866068217043\n",
      "Epoch: 857 Loss: 612.9364967865686\n",
      "Epoch: 858 Loss: 612.5868057324367\n",
      "Epoch: 859 Loss: 612.2375328061819\n",
      "Epoch: 860 Loss: 611.8886771581019\n",
      "Epoch: 861 Loss: 611.5402379419065\n",
      "Epoch: 862 Loss: 611.1922143146952\n",
      "Epoch: 863 Loss: 610.8446054369374\n",
      "Epoch: 864 Loss: 610.4974104724652\n",
      "Epoch: 865 Loss: 610.1506285884386\n",
      "Epoch: 866 Loss: 609.8042589553427\n",
      "Epoch: 867 Loss: 609.4583007469438\n",
      "Epoch: 868 Loss: 609.1127531403014\n",
      "Epoch: 869 Loss: 608.7676153157362\n",
      "Epoch: 870 Loss: 608.4228864568045\n",
      "Epoch: 871 Loss: 608.0785657502938\n",
      "Epoch: 872 Loss: 607.7346523861875\n",
      "Epoch: 873 Loss: 607.3911455576673\n",
      "Epoch: 874 Loss: 607.0480444610943\n",
      "Epoch: 875 Loss: 606.7053482959569\n",
      "Epoch: 876 Loss: 606.3630562648993\n",
      "Epoch: 877 Loss: 606.0211675736851\n",
      "Epoch: 878 Loss: 605.6796814311659\n",
      "Epoch: 879 Loss: 605.3385970492899\n",
      "Epoch: 880 Loss: 604.9979136430551\n",
      "Epoch: 881 Loss: 604.6576304305336\n",
      "Epoch: 882 Loss: 604.3177466328132\n",
      "Epoch: 883 Loss: 603.9782614739994\n",
      "Epoch: 884 Loss: 603.6391741811954\n",
      "Epoch: 885 Loss: 603.300483984497\n",
      "Epoch: 886 Loss: 602.962190116964\n",
      "Epoch: 887 Loss: 602.6242918145856\n",
      "Epoch: 888 Loss: 602.2867883163149\n",
      "Epoch: 889 Loss: 601.9496788640134\n",
      "Epoch: 890 Loss: 601.6129627024332\n",
      "Epoch: 891 Loss: 601.2766390792134\n",
      "Epoch: 892 Loss: 600.9407072448788\n",
      "Epoch: 893 Loss: 600.6051664527929\n",
      "Epoch: 894 Loss: 600.2700159591593\n",
      "Epoch: 895 Loss: 599.9352550230162\n",
      "Epoch: 896 Loss: 599.6008829061916\n",
      "Epoch: 897 Loss: 599.2668988733178\n",
      "Epoch: 898 Loss: 598.9333021918045\n",
      "Epoch: 899 Loss: 598.6000921318131\n",
      "Epoch: 900 Loss: 598.2672679662624\n",
      "Epoch: 901 Loss: 597.9348289707974\n",
      "Epoch: 902 Loss: 597.6027744237794\n",
      "Epoch: 903 Loss: 597.2711036062761\n",
      "Epoch: 904 Loss: 596.939815802037\n",
      "Epoch: 905 Loss: 596.6089102974804\n",
      "Epoch: 906 Loss: 596.2783863817001\n",
      "Epoch: 907 Loss: 595.9482433464149\n",
      "Epoch: 908 Loss: 595.6184804859885\n",
      "Epoch: 909 Loss: 595.2890970973858\n",
      "Epoch: 910 Loss: 594.9600924801815\n",
      "Epoch: 911 Loss: 594.6314659365394\n",
      "Epoch: 912 Loss: 594.3032167711867\n",
      "Epoch: 913 Loss: 593.9753442914184\n",
      "Epoch: 914 Loss: 593.6478478070763\n",
      "Epoch: 915 Loss: 593.3207266305211\n",
      "Epoch: 916 Loss: 592.9939800766457\n",
      "Epoch: 917 Loss: 592.6676074628448\n",
      "Epoch: 918 Loss: 592.3416081089979\n",
      "Epoch: 919 Loss: 592.0159813374688\n",
      "Epoch: 920 Loss: 591.6907264730839\n",
      "Epoch: 921 Loss: 591.3658428431206\n",
      "Epoch: 922 Loss: 591.0413297772917\n",
      "Epoch: 923 Loss: 590.7171866077413\n",
      "Epoch: 924 Loss: 590.3934126690223\n",
      "Epoch: 925 Loss: 590.0700072980948\n",
      "Epoch: 926 Loss: 589.7469698342826\n",
      "Epoch: 927 Loss: 589.4242996193135\n",
      "Epoch: 928 Loss: 589.1019959972541\n",
      "Epoch: 929 Loss: 588.7800583145358\n",
      "Epoch: 930 Loss: 588.4584859199077\n",
      "Epoch: 931 Loss: 588.1372781644603\n",
      "Epoch: 932 Loss: 587.816434401591\n",
      "Epoch: 933 Loss: 587.4959539869881\n",
      "Epoch: 934 Loss: 587.1758362786398\n",
      "Epoch: 935 Loss: 586.8560806367982\n",
      "Epoch: 936 Loss: 586.5366864239868\n",
      "Epoch: 937 Loss: 586.2176530049758\n",
      "Epoch: 938 Loss: 585.8989797467777\n",
      "Epoch: 939 Loss: 585.5806660186252\n",
      "Epoch: 940 Loss: 585.2627111919735\n",
      "Epoch: 941 Loss: 584.9451146404847\n",
      "Epoch: 942 Loss: 584.6278757400094\n",
      "Epoch: 943 Loss: 584.3109938685701\n",
      "Epoch: 944 Loss: 583.9944684063753\n",
      "Epoch: 945 Loss: 583.6782987357822\n",
      "Epoch: 946 Loss: 583.3624842412959\n",
      "Epoch: 947 Loss: 583.047024309563\n",
      "Epoch: 948 Loss: 582.7319183293399\n",
      "Epoch: 949 Loss: 582.4171656915182\n",
      "Epoch: 950 Loss: 582.1027657890684\n",
      "Epoch: 951 Loss: 581.7887180170733\n",
      "Epoch: 952 Loss: 581.4750217726819\n",
      "Epoch: 953 Loss: 581.1616764551183\n",
      "Epoch: 954 Loss: 580.8486814656659\n",
      "Epoch: 955 Loss: 580.5360362076576\n",
      "Epoch: 956 Loss: 580.2237400864637\n",
      "Epoch: 957 Loss: 579.9117925094705\n",
      "Epoch: 958 Loss: 579.6001928861033\n",
      "Epoch: 959 Loss: 579.2889406277754\n",
      "Epoch: 960 Loss: 578.9780351479008\n",
      "Epoch: 961 Loss: 578.6674758618785\n",
      "Epoch: 962 Loss: 578.3572621870887\n",
      "Epoch: 963 Loss: 578.0473935428694\n",
      "Epoch: 964 Loss: 577.7378693505162\n",
      "Epoch: 965 Loss: 577.4286890332685\n",
      "Epoch: 966 Loss: 577.1198520163034\n",
      "Epoch: 967 Loss: 576.8113577267202\n",
      "Epoch: 968 Loss: 576.5032055935393\n",
      "Epoch: 969 Loss: 576.1953950476715\n",
      "Epoch: 970 Loss: 575.8879255219432\n",
      "Epoch: 971 Loss: 575.580796451054\n",
      "Epoch: 972 Loss: 575.2740072715817\n",
      "Epoch: 973 Loss: 574.9675574219763\n",
      "Epoch: 974 Loss: 574.6614463425303\n",
      "Epoch: 975 Loss: 574.3556734754054\n",
      "Epoch: 976 Loss: 574.0502382645839\n",
      "Epoch: 977 Loss: 573.7451401558832\n",
      "Epoch: 978 Loss: 573.440378596945\n",
      "Epoch: 979 Loss: 573.1359530372141\n",
      "Epoch: 980 Loss: 572.8318629279373\n",
      "Epoch: 981 Loss: 572.5281077221571\n",
      "Epoch: 982 Loss: 572.2246868746975\n",
      "Epoch: 983 Loss: 571.9215998421568\n",
      "Epoch: 984 Loss: 571.6188460828965\n",
      "Epoch: 985 Loss: 571.3164250570452\n",
      "Epoch: 986 Loss: 571.0143362264533\n",
      "Epoch: 987 Loss: 570.7125790547408\n",
      "Epoch: 988 Loss: 570.4111530072315\n",
      "Epoch: 989 Loss: 570.1100575509839\n",
      "Epoch: 990 Loss: 569.809292154768\n",
      "Epoch: 991 Loss: 569.5088562890563\n",
      "Epoch: 992 Loss: 569.2087494260169\n",
      "Epoch: 993 Loss: 568.9089710394965\n",
      "Epoch: 994 Loss: 568.6095206050401\n",
      "Epoch: 995 Loss: 568.3103975998393\n",
      "Epoch: 996 Loss: 568.0116015027608\n",
      "Epoch: 997 Loss: 567.7131317943247\n",
      "Epoch: 998 Loss: 567.414987956688\n",
      "Epoch: 999 Loss: 567.1171694736528\n",
      "Weights: [ 2.33637825e+02  1.00656225e+02  7.03802913e+02  1.07342953e+03\n",
      " -9.67511232e+00  1.55638988e+03  7.33668117e+02 -1.02513121e+02\n",
      "  9.30895793e+02  3.81532244e+02  1.52762987e+00  2.87509007e+00\n",
      "  1.34445400e+03  1.32337690e+03  7.20877381e+01  1.04421273e+03\n",
      "  3.61779552e+01  7.64569001e+02  7.61864974e+02  1.29122081e+03\n",
      "  1.48498514e+02 -4.65084937e+00 -8.98681899e+01  7.10543911e+02\n",
      " -7.77150461e+00]\n",
      "Biase: 139.05\n"
     ]
    }
   ],
   "source": [
    "w_out, b_out = gradient_descent(X_train_normalize, Y_train, w, b)\n",
    "print(f\"Weights: {w_out:.2f}\\nBiase: {b_out:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(original, pred):\n",
    "    original_mean = original.mean()\n",
    "    original_std = original.std()\n",
    "    de_normal = pred*original_std+original_mean\n",
    "    return de_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: [ 5235.69 -1221.44 -1102.76   437.94   755.83  1514.26  6158.76    65.79\n",
      "  -398.21  -887.33 -1834.69  2017.08   377.92  -754.31 -1237.41  2041.71\n",
      "  1200.94  3339.56  3721.4   -291.77  1308.51 -1569.07  -322.97  1745.64\n",
      "  2594.06  2581.09  3798.35  3161.22 -1719.71  3595.62  3867.35  5059.68\n",
      "  5783.84  2001.36  1569.25  5056.15 -2166.32  1503.85 -2004.43   811.03\n",
      "  1396.2    839.25  -156.66  4044.99  3470.24  -228.84  3088.68  -153.34\n",
      "  3920.78  1525.64]\n",
      "y_pred: [ 4458.96  -878.88  -631.59  -148.5   1443.33  1888.26  5765.31   578.28\n",
      "  -144.56   261.58 -1405.81  2070.6   -660.28   235.35  -913.58  1834.22\n",
      "   135.43  2266.16  3760.77   198.48  1717.1   -321.63  -205.21  1820.7\n",
      "  1329.4   2134.76  3499.37  2754.98 -1016.64  3156.58  3978.38  3924.21\n",
      "  4590.37  2552.49  1739.63  4257.03 -1265.11  1028.4  -2095.94   803.65\n",
      "  1210.44  1044.61  -221.02  3669.93  1702.89  -252.96  1914.3   1073.\n",
      "  3654.41  1784.81]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(len(Y_train))\n",
    "for i in range(len(X_train)):\n",
    "    y_pred[i] = np.dot(X_train_normalize[i],w_out) + b_out\n",
    "# y_pred = denormalize(Y_train, y_pred)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(f\"Y_train: {Y_train[:50]}\")\n",
    "print(f\"y_pred: {y_pred[:50]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [ 2038.9   4662.36  2462.08  -329.81  1231.59  1324.67  2990.45  4127.52\n",
      "   442.46   685.71  3949.04  1064.6   3097.7   2530.49 -1732.    1271.69\n",
      "  3427.45  -767.61 -2475.27   174.4   2084.4    591.39  2901.37  3352.05\n",
      "  4468.57  1441.33  3622.53  1909.69  -446.39 -1808.64  -371.51   580.7\n",
      "  1800.83  3358.36  -111.54 -1452.31  -843.6   3062.42  1691.08   345.05\n",
      "  2474.87  3025.66  1766.69  -228.22  -682.61  -258.21 -1459.31  1634.62\n",
      "  5887.51  -767.34]\n"
     ]
    }
   ],
   "source": [
    "test_data_normalize = normalize(np.array(test_data))\n",
    "test_prediction = np.zeros(len(test_data))\n",
    "for i in range(len(test_data)):\n",
    "    test_prediction[i] = np.dot(test_data_normalize[i],w_out) + b_out\n",
    "# test_prediction = denormalize(Y_train, y_pred)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(f\"y_pred: {test_prediction[:50]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
