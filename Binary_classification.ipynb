{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Training and Testing dataset using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\DataSets\\binary_classification_train.csv\")\n",
    "df_test = pd.read_csv(r\"D:\\DataSets\\binary_classification_test.csv\")\n",
    "df = df.drop(columns=[\"ID\"])\n",
    "df_test = df_test.drop(columns=[\"ID\"])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Next cell, dataset is shuffled randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shuffle(df_arr):\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(df_arr)\n",
    "    np.random.seed(None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spilliting the training data's features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr = np.array(df)\n",
    "shuffle(df_arr)\n",
    " \n",
    "X_train = df_arr[ : ,0:20]\n",
    "Y_train = df_arr[ : ,20]\n",
    "test = df_test.iloc[:,:].values\n",
    " \n",
    "print(f\"X_Training set shape: {X_train.shape}\")\n",
    "print(f\"Y_Training set shape: {Y_train.shape}\")\n",
    "print(f\"Test set Shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum of the Training set: {np.max(X_train)}\")\n",
    "print(f\"Minimum of the Training set: {np.min(X_train)}\")\n",
    "\n",
    "n_unique, n_counts_train = np.unique(Y_train, return_counts=True)\n",
    "\n",
    "print(f\"Total number of 1 and 0 in Training  set:\\n1: {n_counts_train[1]}\\n0: {n_counts_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since given data set has a very wide range of values, so normilaziation of features of data is crucial.\n",
    "Normalization makes the values to fall in small bracket which advances the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(x):\n",
    "    x_mean = x.mean()\n",
    "    x_std = x.std()\n",
    "    x_normal = (x-x_mean)/x_std\n",
    "    return x_normal\n",
    "\n",
    "X_train_normal = Normalize(X_train)\n",
    "test_normal = Normalize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid activation function is used so that all the values fals between 0 and 1.\n",
    "This activation function is helpfull in binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In  next cell, Gradient of cost function is being calculated.\n",
    "Cost function = -(1/m)(summation)(Y[i]*log(sigmoid(z)) + (i-Y[i])*log(1-sigmoid(z)))\n",
    "derivative w.r.t w = (sigmoid(z) - Y[i])*X[i]\n",
    "derivative w.r.t b = (sigmoid(z) - Y[i])\n",
    "\n",
    "here, z = w1x1 + w2x2 + w3x3 +......+ wnxn + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient(X, Y, w, b):\n",
    "    m, n = X.shape\n",
    "    djdw = np.zeros(n)\n",
    "    djdb = 0\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        z = sigmoid(z)\n",
    "        err = z - Y[i]\n",
    "        loss += -(Y[i]*np.log(z) + (1-Y[i])*np.log(1-z))\n",
    "        djdw += err*X[i]\n",
    "        djdb += err\n",
    "    djdw /= (m)\n",
    "    djdb /= (m)\n",
    "    loss /= m\n",
    "    return djdw, djdb, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is initalization,\n",
    "Initalize weights with random values (size of weights must be equal to number of features).\n",
    "Initalize learning rate = 0.01 (its just a convenntional values other values can also be tried and used but it must be small).\n",
    "Epoch is no. of times training is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = X_train.shape\n",
    "w = np.random.rand(n)\n",
    "b = 0\n",
    "learning_rate = 0.01\n",
    "Epoch = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is, Gradient Descent updating the weights and biases after each Epoch and printing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient_descent(X, Y, w, b):\n",
    "    m,n = X.shape\n",
    "    for i in range(Epoch):\n",
    "        djdw, djdb, loss = logistic_gradient(X, Y, w, b)\n",
    "        w -= learning_rate*djdw\n",
    "        b -= learning_rate*djdb\n",
    "        print(f\"Epoch: {i+1} Loss: {loss}\")\n",
    "    return w, b\n",
    "\n",
    "w_out, b_out = logistic_gradient_descent(X_train_normal, Y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the Refined weights and biases.\n",
    "It will be used on test set for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weights: {w_out}\\n Biases: {b_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is just to check whether weights and bias are correct at some satisfactory level or not.\n",
    "Here, extracting the data from the training set, making predictions on that data using trained weights and bias and finally checking whether predicted value is matching with Actual value or not and accordingly printing the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test():\n",
    "    x_test = X_train_normal[38000:48000]\n",
    "    y_test = Y_train[38000:48000]\n",
    "    prediction = np.zeros(len(x_test))\n",
    "    for i in range(len(x_test)):\n",
    "        prediction[i] = np.dot(x_test[i],w_out) + b_out\n",
    "    prediction = np.where(prediction>=0.5, 1, 0)\n",
    "    accuracy = np.mean(prediction==y_test)*100\n",
    "    print(f\"Prediction: {prediction}\\nAccuracy: {accuracy:.2f}\")\n",
    "    return\n",
    "\n",
    "sample_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, weights and bias are used on the test set and the predictions are made.\n",
    "Counting and printing the number of 0's and 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros(len(test))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    predictions[i] = np.dot(test_normal[i], w_out) + b_out\n",
    "\n",
    "predictions = np.where(predictions>=0.5, 1, 0)\n",
    "n_unique, n_counts_test = np.unique(predictions, return_counts=True)\n",
    "\n",
    "print(f\"Shape of predictions: {predictions.shape}\")\n",
    "print(f\"Predictions on test set is: {predictions}\")\n",
    "print(f\"Total number of 1 and 0 in Predictions :\\n1: {n_counts_test[1]} 0: {n_counts_test[0]}\")\n",
    "print(f\"1st 50 predictions on set: {predictions[:50]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
