{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"D:\\DataSets\\nn_train.csv\")\n",
    "df_test = pd.read_csv(r\"D:\\DataSets\\nn_test.csv\")\n",
    "df_train = df_train.drop(columns=['ID'])\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(df_arr):\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(df_arr)\n",
    "    np.random.seed(None)\n",
    "    return\n",
    "\n",
    "df_train = np.array(df_train)\n",
    "shuffle(df_train)\n",
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[:,:1024]\n",
    "binary_label = df_train[:,1024]\n",
    "class_label = df_train[:,1025]\n",
    "\n",
    "print(f\"Shape of Tarining set: {X_train.shape}\")\n",
    "print(f\"Shape of Binary Label: {binary_label.shape}\")\n",
    "print(f\"Shape of class label: {class_label.shape}\")\n",
    "print(f\"Unique values in Binary label is: {np.unique(binary_label)}\")\n",
    "print(f\"Unique values in Class label is: {np.unique(class_label)}\")\n",
    "print(f\"Maximum value of data is: {np.max(X_train)}\")\n",
    "print(f\"Minimum value of data is: {np.min(X_train)}\")\n",
    "\n",
    "n_unique, n_counts_train = np.unique(binary_label, return_counts=True)\n",
    "\n",
    "print(f\"Total number of 1 and 0 in Training  set:\\n1: {n_counts_train[1]}\\n0: {n_counts_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # x_mean = x.mean()\n",
    "    # x_std = x.std()\n",
    "    # x_normal = (x-x_mean)/x_std\n",
    "    return x/255\n",
    "\n",
    "X_train_normalize = normalize(X_train)\n",
    "X_train_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    return exp_z/(np.sum(exp_z, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.where(y_pred<=1e-12, 1e-12, y_pred)\n",
    "    loss = np.sum(y_true*np.log(y_pred), axis=1)\n",
    "    cost = -np.mean(loss)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    labels = labels.astype(int)\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels= len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
    "    for i in range(n_labels):\n",
    "        one_hot_encode[i,labels[i]] = 1\n",
    "    return one_hot_encode\n",
    "\n",
    "binary_one_hot = one_hot_encode(binary_label)\n",
    "\n",
    "class_label = class_label-1\n",
    "class_one_hot = one_hot_encode(class_label)\n",
    "class_label = class_label + 1\n",
    "class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_train[0])\n",
    "m = len(X_train)\n",
    "\n",
    "a1 = 512\n",
    "a2 = 256\n",
    "a3 = 128\n",
    "a4 = 64\n",
    "binary_op = 2\n",
    "class_op = 10\n",
    "\n",
    "w1 = np.random.rand(n, a1)*np.sqrt(1/n)\n",
    "w2 = np.random.rand(a1, a2)*np.sqrt(1/a1)\n",
    "w3 = np.random.rand(a2, a3)*np.sqrt(1/a2)\n",
    "w4 = np.random.rand(a3, a4)*np.sqrt(1/a3)\n",
    "w_binary = np.random.rand(a4, binary_op)*np.sqrt(1/a4)\n",
    "w_class = np.random.rand(binary_op, class_op)*np.sqrt(1/a4)\n",
    "\n",
    "b1 = np.zeros((m, a1))\n",
    "b2 = np.zeros((m, a2))\n",
    "b3 = np.zeros((m, a3))\n",
    "b4 = np.zeros((m, a4))\n",
    "b_binary = np.zeros((m, binary_op))\n",
    "b_class = np.zeros((m, class_op))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 1000\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(0, m, batch_size):\n",
    "        batch_x = X_train_normalize[j:j+batch_size]\n",
    "        binary_one_hot_batch = binary_one_hot[j:j+batch_size]\n",
    "        class_one_hot_batch = class_one_hot[j:j+batch_size]\n",
    "        binary_Y = binary_label[j:j+batch_size]\n",
    "        class_Y = class_label[j:j+batch_size]\n",
    "\n",
    "        #forward propogation\n",
    "        z1 = np.dot(batch_x, w1) + b1[j:j+batch_size]\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(a1, w2) + b2[j:j+batch_size]\n",
    "        a2 = sigmoid(z2)\n",
    "        z3 = np.dot(a2, w3) + b3[j:j+batch_size]\n",
    "        a3 = sigmoid(z3)\n",
    "        z4 = np.dot(a3, w4) + b4[j:j+batch_size]\n",
    "        a4 = sigmoid(z4)\n",
    "        z5 = np.dot(a4, w_binary) + b_binary[j:j+batch_size]\n",
    "        binary_op = softmax(z5)\n",
    "        z6 = np.dot(binary_op, w_class) + b_class[j:j+batch_size]\n",
    "        class_op  = softmax(z6)\n",
    "       \n",
    "        #backword propogation\n",
    "        class_err = class_op - class_one_hot_batch\n",
    "        binary_err = class_err.dot(w_class.T)*sigmoid_derivative(binary_op)\n",
    "        err_a4 = binary_err.dot(w_binary.T)*sigmoid_derivative(a4)\n",
    "        err_a3 = err_a4.dot(w4.T)*sigmoid_derivative(a3)\n",
    "        err_a2 = err_a3.dot(w3.T)*sigmoid_derivative(a2)\n",
    "        err_a1 = err_a2.dot(w2.T)*sigmoid_derivative(a1)\n",
    "\n",
    "        #weight updation\n",
    "        w_class -= learning_rate*(binary_op.T).dot(class_err)\n",
    "        w_binary -= learning_rate*(a4.T).dot(binary_err)\n",
    "        w4 -= learning_rate*(a3.T).dot(err_a4)\n",
    "        w3 -= learning_rate*(a2.T).dot(err_a3)\n",
    "        w2 -= learning_rate*(a1.T).dot(err_a2)\n",
    "        w1 -= learning_rate*(batch_x.T).dot(err_a1)\n",
    "        \n",
    "        #biaes updation\n",
    "        b_class[j:j+batch_size] -= learning_rate*class_err\n",
    "        b_binary[j:j+batch_size] -= learning_rate*binary_err\n",
    "        b4[j:j+batch_size] -= learning_rate*err_a4\n",
    "        b3[j:j+batch_size] -= learning_rate*err_a3\n",
    "        b2[j:j+batch_size] -= learning_rate*err_a2\n",
    "        b1[j:j+batch_size] -= learning_rate*err_a1\n",
    "\n",
    "    binary_loss = cross_entropy_loss(binary_one_hot_batch, binary_op)\n",
    "    class_loss = cross_entropy_loss(class_one_hot_batch, class_op)\n",
    "    binary_prediction = np.argmax(binary_op, axis=1)\n",
    "    class_prediction = np.argmax(class_op, axis=1)\n",
    "    binary_accuracy = np.mean((binary_prediction+1)==binary_Y)*100\n",
    "    class_accuracy = np.mean(class_prediction == class_Y)*100\n",
    "    print(f\"Epoch: {1+i} Binary Loss: {binary_loss:.3f} Binary_Accuracy: {binary_accuracy:.3f} Class_loss: {class_loss:.3f} Class_Accuracy: {class_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = sigmoid(np.dot(df_test,w1) + b1[:len(df_test)])\n",
    "a2 = sigmoid(np.dot(a1, w2) + b2[:len(df_test)])\n",
    "a3 = softmax(np.dot(a2, w3) + b3[:len(df_test)])\n",
    "a4 = sigmoid(np.dot(a3, w4) + b4[:len(df_test)])\n",
    "binary_test_prediction = softmax(np.dot(a4, w_binary) + b_binary[:len(df_test)])\n",
    "class_test_prediction = softmax(np.dot(binary_prediction, w_class) + b_class[:len(df_test)])\n",
    "\n",
    "binary_prediction = np.argmax(binary_test_prediction, axis=1)\n",
    "class_prediction = np.argmax(class_test_prediction, axis=1)\n",
    "\n",
    "print(f\"Binary Prediction: {binary_test_prediction}\")\n",
    "print(f\"Class Prediction: {class_test_prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
